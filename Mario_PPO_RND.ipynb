{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19939a9c-03f0-4150-9477-9109e9735319",
   "metadata": {
    "id": "19939a9c-03f0-4150-9477-9109e9735319"
   },
   "source": [
    "# Install librarys\n",
    "Install packages need to train mario agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d",
   "metadata": {
    "id": "44a21d2d-dccd-447b-84e8-37c55db5ef2d"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install gym-super-mario-bros==7.4.0\n",
    "!pip install gym==0.25.2\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install imageio\n",
    "!pip install torchvision\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db125d3",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f",
   "metadata": {
    "id": "7144ed99-8f5f-489c-b6b4-2ce51f9b9d8f"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import random, os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "#import multiprocessing as mp\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891b1e0",
   "metadata": {},
   "source": [
    "# Create hyperparammeters\n",
    "Config hyperparammeters, just change it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95999c-5111-4054-9256-45cd206a697e",
   "metadata": {
    "id": "4a95999c-5111-4054-9256-45cd206a697e"
   },
   "outputs": [],
   "source": [
    "#class DictWrapper create by Chatgpt\n",
    "class DictWrapper:\n",
    "    def __init__(self, dictionary):\n",
    "        self._dict = dictionary\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        if item in self._dict:\n",
    "            return self._dict[item]\n",
    "        else:\n",
    "            raise AttributeError(f\"'DictWrapper' object has no attribute '{item}'\")\n",
    "\n",
    "config = {\n",
    "    'num_envs': 32,\n",
    "    'save_model_step': int(1e5),\n",
    "    'save_figure_step': 756,\n",
    "    'learn_step': 756,\n",
    "    'total_step_or_episode': 'step',\n",
    "    'total_step': int(5e6),\n",
    "    'total_episode': None,\n",
    "    'batch_size': 256,\n",
    "    'save_dir': \"\",\n",
    "    'gamma': 0.99,\n",
    "    'gamma_int': 0.99,\n",
    "    'learning_rate': 7e-5,\n",
    "    'state_dim': (4, 84, 84),\n",
    "    'action_dim': 12,#12 for complex, 7 for simple\n",
    "    'entropy_coef': 0.05,\n",
    "    'V_coef': 0.5,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'clip_param': 0.2,\n",
    "    'num_epoch': 10,\n",
    "    'world': 8,\n",
    "    'stage': 4,\n",
    "    'action_type': 'complex',\n",
    "    'is_normalize_advantage': False,\n",
    "    'V_loss_type': \"mse\", #\"huber\"\n",
    "    'target_kl': 0.05,\n",
    "    'gae_lambda': 0.95,\n",
    "    'int_adv_coef': 1,\n",
    "    'ext_adv_coef': 2\n",
    "}\n",
    "\n",
    "config = DictWrapper(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b43f43",
   "metadata": {},
   "source": [
    "# Define environment\n",
    "## Create a custom environment, We need:\n",
    "- SkipFrame: Because the episode is very long, we only need to repeat actions sometimes in this environment. We repeat each action 4 times and skip the first 3 frames (returning the 4th frame). We also sum the rewards from all 4 frames.\n",
    "- GrayScaleResizeObservation: Convert the state to grayscale (from RGB to a gray image) and resize it to 84x84 pixels.\n",
    "- NoopResetEnv: When resetting the environment, we perform random actions before starting the environment. This is similar to the Atari strategy. When resetting, we randomly choose num_noops actions between 0 and noop_max and perform num_noops random actions. If the random actions lead to a terminal state, we reset and continue performing random actions. I set noop_max to 30, similar to Atari.\n",
    "- CustomRewardAndDoneEnv\n",
    "    - I noticed that many people train Mario using this custom reward system, so I copied it. The system adds 50 reward points if the agent solves the stage and subtracts 50 reward points if the agent dies. The reward is divided by 10. I set done to True if Mario dies, instead of the default setting where Mario loses all lives.\n",
    "    - Stage 4-2: Subtract 50 reward points if Mario moves on top of the map (y_pos >= 255).\n",
    "    - Stages 4-4 and 7-4: Set done = True when Mario goes the wrong way and subtract 50 reward points as a penalty. If Mario takes the correct path but the map still loops (a known bug), I set done = True but do not apply a penalty.\n",
    "    - Stage 8-4: Set done = True when Mario goes the wrong way and subtract 50 reward points as a penalty (similar to stages 4-4 and 7-4). This map has a particularly difficult section where Mario needs to find a hidden brick. I apply a -100 penalty in this part (info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500).\n",
    "    - Stage 8-4: Add 100 reward points when Mario successfully takes the correct path.\n",
    "    - Stages 4-4 and 8-4: give a -0.1 reward every step.\n",
    "\n",
    "\n",
    "## About reward system:\n",
    "- Set done to True when Mario dies: This is the most important aspect because, in the default reward system, Mario still gains a reward by just moving right. If Mario dies, the agent doesn't lose total rewards and can continue moving right (in the new life) to get more rewards. This is the easiest way for the agent to earn rewards, and it can learn to exploit this trick.\n",
    "- Penalty of -50 reward when Mario dies: This is necessary to speed up Mario's training. Without this penalty, the agent may struggle to complete more difficult stages.\n",
    "- Reward of 50 when reaching the flag: This encourages Mario to train faster and overcome difficult sections in harder stages.\n",
    "- Changing the penalty and flag reward to more or less than 50 doesn't make a significant difference, so I haven't changed it.\n",
    "- Divide rewards by 10: I believe this reduces the total rewards and helps the agent learn a better strategy, but I'm not entirely sure how necessary this is. I simply followed an existing approach.\n",
    "- With Stage 4-2: I noticed that the agent can earn more rewards when Mario goes to the warp zone, but Mario can't win this stage using the warp zone because the reward system gives negative rewards when Mario moves left. Therefore, I added a penalty when Mario moves to the top of the map.\n",
    "- Use FrameStack to stack the latest 4 frames as observation.\n",
    "- With Stage 4-4 and 7-4:\n",
    "    - Since this map has a wrong path, Mario can enter a loop where the reward increases indefinitely. To prevent this, I set done = True and assign a negative penalty reward.\n",
    "    - I also give a negative reward to prevent Mario from taking the wrong path.\n",
    "    - Another strategy is to give a negative reward without setting done = True (as in 4-2). However, this strategy doesn't work due to a bug in this map.\n",
    "    - Even when Mario is on the correct path, sometimes he still enters the loop. To handle this, I set done = True every time Mario enters the loop (checked by x_pos and max_x_pos).\n",
    "- Stage 4-4: Assign a -0.1 reward for every step: This prevents Mario from getting stuck. This map has a section where Mario needs to move left, but moving left incurs a negative reward in the default system. If Mario moves right, he takes the wrong path, causing the episode to end with a negative reward. To keep Mario moving, I added a negative reward for every step.\n",
    "- Stage 8-4:\n",
    "    - Assign a -0.1 reward for every step: This prevents Mario from getting stuck. This encourages Mario to move when stuck at a particular section (info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500). Since moving right can lead Mario down the wrong path, the agent often learns to do nothing to avoid losing rewards. This penalty encourages exploration to find the hidden brick.\n",
    "    - Sams as in 4-4 and 7-4, I set done = True and assign a -50 penalty when the agent moves in the wrong direction. At the hardest part (info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500), I increased the penalty to -100.\n",
    "    - Add a 50 reward when the agent goes the correct way (avoiding the wrong path).\n",
    "    - Another strategy is giving a +50 reward when the agent finds the hidden brick. I've tried both methods, and they both work.\n",
    "    - Since this map has many locations with overlapping x_pos values (especially in the underwater section where x_pos is reset to 1), be careful when modifying the custom reward system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da",
   "metadata": {
    "id": "ba268b64-6d59-4682-ac6e-f4118bd2d6da"
   },
   "outputs": [],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{config.world}-{config.stage}-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{config.world}-{config.stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "print(env.action_space)\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "class GrayScaleResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.current_state = observation\n",
    "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        observation = observation.astype(np.uint8)#.reshape(-1, observation.shape[0], observation.shape[1])\n",
    "        return observation\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super(NoopResetEnv, self).__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        noops = np.random.randint(0, self.noop_max, (1, ))[0]\n",
    "        for _ in range(noops):\n",
    "            action = self.env.action_space.sample()\n",
    "            obs, _, done, _, _ = self.env.step(action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        obs, reward, done, trunk, info = self.env.step(ac)\n",
    "        return obs, reward, done, trunk, info\n",
    "\n",
    "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, world=1, stage=1):\n",
    "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
    "        self.current_score = 0\n",
    "        self.current_x = 0\n",
    "        self.current_x_count = 0\n",
    "        self.max_x = 0\n",
    "        self.world = world\n",
    "        self.stage = stage\n",
    "        if self.world == 8 and self.stage == 4:\n",
    "            self.sea_map = False\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_score = 0\n",
    "        self.current_x = 0\n",
    "        self.current_x_count = 0\n",
    "        self.max_x = 0\n",
    "        if self.world == 8 and self.stage == 4:\n",
    "            self.sea_map = False\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        if (info['x_pos'] - self.current_x) == 0:\n",
    "            self.current_x_count += 1\n",
    "        else:\n",
    "            self.current_x_count = 0\n",
    "        if info[\"flag_get\"]:\n",
    "            reward += 50\n",
    "            done = True\n",
    "        if done and info[\"flag_get\"] == False and info[\"time\"] != 0:\n",
    "            reward -= 50\n",
    "            done = True\n",
    "        self.current_x = info[\"x_pos\"]\n",
    "\n",
    "        if self.world == 7 and self.stage == 4:\n",
    "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
    "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
    "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
    "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
    "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
    "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
    "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191):\n",
    "                reward -= 50\n",
    "                done = True\n",
    "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
    "                done = True\n",
    "        if self.world == 4 and self.stage == 4:\n",
    "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
    "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
    "                reward = -50\n",
    "                done = True\n",
    "            if done == False and info[\"x_pos\"] < self.max_x - 100:\n",
    "                done = True\n",
    "            if done == False:\n",
    "                reward -= 0.1\n",
    "        if self.world == 4 and self.stage == 2 and done == False and info['y_pos'] >= 255:\n",
    "            reward -= 50\n",
    "        if self.world == 8 and self.stage == 4:\n",
    "            if info[\"x_pos\"] > 2440 and info[\"x_pos\"] <= 2500:\n",
    "                done = True\n",
    "                reward -= 100\n",
    "            if info[\"x_pos\"] >= 3675 and info[\"x_pos\"] <= 3700:\n",
    "                done = True\n",
    "                reward -= 50\n",
    "\n",
    "            if info[\"x_pos\"] < self.max_x - 200:\n",
    "                if self.max_x >= 1250 and self.max_x <= 1310: #solved bug because x_pos duplicated\n",
    "                    if info[\"x_pos\"] >= 320:\n",
    "                        done = True\n",
    "                        reward = -50\n",
    "                elif info[\"x_pos\"] >= 312-5 and info[\"x_pos\"] <= 312+5:\n",
    "                    done = True\n",
    "                    reward = -50\n",
    "                elif info[\"x_pos\"] >= 56-5 and info[\"x_pos\"] <= 56-5 and self.max_x > 3650 and self.sea_map == False:\n",
    "                    reward += 50\n",
    "                    self.sea_map = True\n",
    "            if info[\"x_pos\"] > self.max_x + 100:\n",
    "                reward += 50\n",
    "            if done == False:\n",
    "                reward -= 0.1\n",
    "        self.max_x = max(self.max_x, self.current_x)\n",
    "        self.current_score = info[\"score\"]\n",
    "\n",
    "        return state, reward / 10., done, trunc, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8f56c",
   "metadata": {},
   "source": [
    "# Create MultipleEnvironments\n",
    "MultipleEnvironments use multi-processing to parallel running.\n",
    "\n",
    "Because in the training process, we need to reset the environment when the agent reaches the terminal state. But if we will do it in parallel, then I don't want to check each environment and reset (by loop) or create a new function that parallels check and reset all environments. Then I reset the environment if done = True in step function and set next_state = env.reset(). Then in training, we just set state = next_state (next_state is reset state if done = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y2mcbU_JvQwI",
   "metadata": {
    "id": "y2mcbU_JvQwI"
   },
   "outputs": [],
   "source": [
    "#modify from https://github.com/uvipen/Super-mario-bros-PPO-pytorch/blob/master/src/env.py\n",
    "def create_env(world, stage, action_type, test=False):\n",
    "    if gym.__version__ < '0.26':\n",
    "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", new_step_api=True)\n",
    "    else:\n",
    "        env = gym_super_mario_bros.make(f\"SuperMarioBros-{world}-{stage}-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "    if action_type == \"right\":\n",
    "        action_type = RIGHT_ONLY\n",
    "    elif action_type == \"simple\":\n",
    "        action_type = SIMPLE_MOVEMENT\n",
    "    else:\n",
    "        action_type = COMPLEX_MOVEMENT\n",
    "\n",
    "    env = JoypadSpace(env, action_type)\n",
    "\n",
    "    if test == False:\n",
    "        env = NoopResetEnv(env)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = CustomRewardAndDoneEnv(env, world, stage)\n",
    "    env = GrayScaleResizeObservation(env, shape=84)\n",
    "    if gym.__version__ < '0.26':\n",
    "        env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "    else:\n",
    "        env = FrameStack(env, num_stack=4)\n",
    "    return env\n",
    "\n",
    "class MultipleEnvironments:\n",
    "    def __init__(self, world, stage, action_type, num_envs):\n",
    "        self.agent_conns, self.env_conns = zip(*[mp.Pipe(duplex=True) for _ in range(num_envs)])\n",
    "        self.envs = [create_env(world, stage, action_type) for _ in range(num_envs)]\n",
    "\n",
    "        for index in range(num_envs):\n",
    "            process = mp.Process(target=self.run, args=(index,))\n",
    "            process.start()\n",
    "            self.env_conns[index].close()\n",
    "\n",
    "    def run(self, index):\n",
    "        self.agent_conns[index].close()\n",
    "        while True:\n",
    "            request, action = self.env_conns[index].recv()\n",
    "            if request == \"step\":\n",
    "                next_state, reward, done, trunc, info = self.envs[index].step(action)\n",
    "                if done:\n",
    "                    next_state = self.envs[index].reset()\n",
    "                self.env_conns[index].send((next_state, reward, done, trunc, info))\n",
    "            elif request == \"reset\":\n",
    "                self.env_conns[index].send(self.envs[index].reset())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def step(self, actions):\n",
    "        [agent_conn.send((\"step\", act)) for agent_conn, act in zip(self.agent_conns, actions)]\n",
    "        next_states, rewards, dones, truncs, infos = zip(*[agent_conn.recv() for agent_conn in self.agent_conns])\n",
    "        return next_states, rewards, dones, truncs, infos\n",
    "\n",
    "    def reset(self):\n",
    "        [agent_conn.send((\"reset\", None)) for agent_conn in self.agent_conns]\n",
    "        states = [agent_conn.recv() for agent_conn in self.agent_conns]\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8722899",
   "metadata": {},
   "source": [
    "# RunningMeanStd\n",
    "RunningMeanStd is used to normalize observation for random network distillation. Please view at [random-network-distillation-pytorch](https://github.com/jcwleo/random-network-distillation-pytorch/blob/master/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4e4dd-8588-4ba7-a401-a834de8f9fb4",
   "metadata": {
    "id": "77f4e4dd-8588-4ba7-a401-a834de8f9fb4"
   },
   "outputs": [],
   "source": [
    "#https://github.com/jcwleo/random-network-distillation-pytorch/blob/master/utils.py\n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=(), device=\"cpu\"):\n",
    "        self.mean = torch.tensor(np.zeros(shape, 'float32'))#.to(device)\n",
    "        self.var = torch.tensor(np.ones(shape, 'float32'))#.to(device)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        x = x.float()\n",
    "        batch_mean = x.mean(0)\n",
    "        batch_var = torch.var(x, 0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "        m_a = self.var * (self.count)\n",
    "        m_b = batch_var * (batch_count)\n",
    "        M2 = m_a + m_b + torch.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        new_var = M2 / (self.count + batch_count)\n",
    "\n",
    "        new_count = batch_count + self.count\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.count = new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d88a35",
   "metadata": {},
   "source": [
    "# Create memory\n",
    "Memory just save all info we need to train and return all stored info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736a4f8-d863-4bed-b290-3812f6c43eae",
   "metadata": {
    "id": "1736a4f8-d863-4bed-b290-3812f6c43eae"
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, num_envs):\n",
    "        self.num_envs = num_envs\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.next_states = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.logits = []\n",
    "        self.values = []\n",
    "        self.values_int = []\n",
    "\n",
    "    def save(self, state, action, reward, next_state, done, logit, value, value_int):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.next_states.append(next_state)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.logits.append(logit)\n",
    "        self.values.append(value)\n",
    "        self.values_int.append(value_int)\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.next_states = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.logits = []\n",
    "        self.values = []\n",
    "        self.values_int = []\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.states, self.actions, self.next_states, self.rewards, self.dones, self.logits, self.values, self.values_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10dad7",
   "metadata": {},
   "source": [
    "# Create agent\n",
    "The agent includes 5 main functions:\n",
    "## train\n",
    "train function train agent via many episodes:\n",
    "- Reset the first state.\n",
    "- loop until the agent wins this stage or reaches the maximum episode/step:\n",
    "    - predict value, value_int, logit for the current state\n",
    "    - sample action from logit with category distribution (select_action function)\n",
    "    - log all info to memory\n",
    "    - train agent every learn_step (learn function)\n",
    "    - eval agent every save_figure_step (save_figure function)\n",
    "    - set state = next_state (I reset environment when agent reach terminal state then next_state is first state if done=True)\n",
    "\n",
    "## select_action\n",
    "this function sample action from logit:\n",
    "- just convert logit to probability: policy = F.softmax(logits, dim=1)\n",
    "- create distribution from probability: distribution = torch.distributions.Categorical(policy)\n",
    "- sample action from distribution: actions = distribution.sample()\n",
    "\n",
    "## save_figure\n",
    "this function eval agent and saves agent/video if the agent yields better total rewards:\n",
    "- reset the environment.\n",
    "- loop until the agent reaches the terminal state.\n",
    "    - predict logit from model\n",
    "    - get action = argmax (logit)\n",
    "    - environment do this action to get next_state, reward, info, done\n",
    "    - if total_reward > best test total reward or agent complete this stage, I save model and video.\n",
    "    - if agent completes this state, we stop training.\n",
    "\n",
    "## compure_reward_int\n",
    "this function used to calculate intrinsic rewards:\n",
    "- first, we need normalize observation before calculate reward (by observation running mean std)\n",
    "- second, we need get features of next_state from target model and predict model\n",
    "- then, calculate intrinsic rewards = MSE between features of target model and predict model \n",
    "- finally, we need normalzie intrinsic rewards (because I use min-max normalization, we need calculate all intrinsic reward for all next_states in memory before normalize, than I normalize it in learn function)\n",
    "\n",
    "## learn\n",
    "this function trains agent from the experiment saved in memory\n",
    "- get all the info from memory\n",
    "- calculate intrinsic rewards\n",
    "- normalize intrinsic rewards\n",
    "- calculate td (lambda) target and gae advantages\n",
    "- train num_epoch epochs:\n",
    "    - shuffle data\n",
    "    - train with each batch data:\n",
    "        - calculate loss\n",
    "        - norm gradient\n",
    "        - update model from loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d-yL01WMumqF",
   "metadata": {
    "id": "d-yL01WMumqF"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, envs, world, stage, action_type, num_envs, state_dim, action_dim, save_dir, save_model_step,\n",
    "                 save_figure_step, learn_step, total_step_or_episode, total_step, total_episode, model,\n",
    "                 target_model, predict_model, gamma, gamma_int, learning_rate, entropy_coef, V_coef, max_grad_norm,\n",
    "                 clip_param, batch_size, num_epoch, is_normalize_advantage, V_loss_type, target_kl, gae_lambda, int_adv_coef,\n",
    "                 ext_adv_coef, device):\n",
    "        self.world = world\n",
    "        self.stage = stage\n",
    "        self.action_type = action_type\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.learn_step = learn_step\n",
    "        self.total_step_or_episode = total_step_or_episode\n",
    "        self.total_step = total_step\n",
    "        self.total_episode = total_episode\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.save_model_step = save_model_step\n",
    "        self.save_figure_step = save_figure_step\n",
    "\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = envs\n",
    "        self.model = model.to(self.device)\n",
    "        self.target_model = target_model.to(self.device)\n",
    "        self.predict_model = predict_model.to(self.device)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.gamma_int = gamma_int\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.V_coef = V_coef\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.feature_optimizer = torch.optim.Adam(self.predict_model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.clip_param = clip_param\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epoch = num_epoch\n",
    "\n",
    "        self.memory = Memory(self.num_envs)\n",
    "        self.is_completed = False\n",
    "\n",
    "        self.env = None\n",
    "        self.max_test_score = -1e9\n",
    "        self.is_normalize_advantage = is_normalize_advantage\n",
    "        self.V_loss_type = V_loss_type\n",
    "        self.target_kl = target_kl\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.int_adv_coef = int_adv_coef\n",
    "        self.ext_adv_coef = ext_adv_coef\n",
    "\n",
    "        # I just log 1000 lastest update and print it to log.\n",
    "        self.V_loss = np.zeros((1000,)).reshape(-1)\n",
    "        self.V_int_loss = np.zeros((1000, )).reshape(-1)\n",
    "        self.P_loss = np.zeros((1000,)).reshape(-1)\n",
    "        self.E_loss = np.zeros((1000,)).reshape(-1)\n",
    "        self.approx_kl_divs = np.zeros((1000,)).reshape(-1)\n",
    "        self.total_loss = np.zeros((1000,)).reshape(-1)\n",
    "        self.loss_index = 0\n",
    "        self.len_loss = 0\n",
    "\n",
    "        self.obs_rms = RunningMeanStd(shape=(1, 1, 84, 84), device=self.device)\n",
    "\n",
    "    def save_figure(self, is_training = False):\n",
    "        # test current model and save model/figure if model yield best total rewards.\n",
    "        # create env for testing, reset test env\n",
    "        if self.env is None:\n",
    "            self.env = create_env(self.world, self.stage, self.action_type, True)\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        images = []\n",
    "        total_reward = 0\n",
    "        total_step = 0\n",
    "        num_repeat_action = 0\n",
    "        old_action = -1\n",
    "\n",
    "        episode_time = datetime.now()\n",
    "\n",
    "        # play 1 episode, just get loop action with max probability from model until the episode end.\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logit, value, value_in = self.model(torch.tensor(np.array(state), dtype = torch.float, device = self.device).unsqueeze(0))\n",
    "            action = logit.argmax(-1).item()\n",
    "            next_state, reward, done, trunc, info = self.env.step(action)\n",
    "            state = next_state\n",
    "            img = Image.fromarray(self.env.current_state)\n",
    "            images.append(img)\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "\n",
    "            if action == old_action:\n",
    "                num_repeat_action += 1\n",
    "            else:\n",
    "                num_repeat_action = 0\n",
    "            old_action = action\n",
    "            if num_repeat_action == 200:\n",
    "                break\n",
    "\n",
    "        #logging, if model yield better result, save figure (test_episode.mp4) and model (best_model.pth)\n",
    "        if is_training:\n",
    "            f_out = open(f\"logging_test.txt\", \"a\")\n",
    "            f_out.write(f'episode_reward: {total_reward:.4f} episode_step: {total_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_v_int: {(self.V_int_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} approx_kl_div: {(self.approx_kl_divs.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time}\\n')\n",
    "            f_out.close()\n",
    "\n",
    "        if total_reward > self.max_test_score or info['flag_get']:\n",
    "            imageio.mimsave('test_episode.mp4', images)\n",
    "            self.max_test_score = total_reward\n",
    "            if is_training:\n",
    "                torch.save(self.model.state_dict(), f\"best_model.pth\")\n",
    "\n",
    "        if info['flag_get']:\n",
    "            self.is_completed = True\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), f\"model_{self.current_step}.pth\")\n",
    "\n",
    "    def load_model(self, model_path = None):\n",
    "        if model_path is None:\n",
    "            model_path = f\"model_{self.current_step}.pth\"\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def update_loss_statis(self, loss_p, loss_v, loss_v_int, loss_e, loss, approx_kl_div):\n",
    "        # update loss for logging, just save 1000 latest updates.\n",
    "        self.V_loss[self.loss_index] = loss_v\n",
    "        self.V_int_loss[self.loss_index] = loss_v_int\n",
    "        self.P_loss[self.loss_index] = loss_p\n",
    "        self.E_loss[self.loss_index] = loss_e\n",
    "        self.total_loss[self.loss_index] = loss\n",
    "        self.approx_kl_divs[self.loss_index] = approx_kl_div\n",
    "        self.loss_index = (self.loss_index + 1)%1000\n",
    "        self.len_loss = min(self.len_loss+1, 1000)\n",
    "\n",
    "    def select_action(self, states):\n",
    "        # select action when training, we need use Categorical distribution to make action base on probability from model\n",
    "        states = torch.tensor(np.array(states), device = self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, Values, values_int = self.model(states)\n",
    "            policy = F.softmax(logits, dim=1)\n",
    "            distribution = torch.distributions.Categorical(policy)\n",
    "            actions = distribution.sample().cpu().numpy().tolist()\n",
    "        return actions, logits, Values, values_int\n",
    "\n",
    "    def compure_reward_int(self, next_state):\n",
    "        # compute intrinsic rewards = MSE between features of target model and predict model \n",
    "        # we need normalize observation before calculate reward (by observation running mean std)\n",
    "        next_state = torch.tensor(np.array(next_state), device = self.device)\n",
    "        next_state = next_state[:, 3, :, :].reshape(-1, 1, next_state.shape[2], next_state.shape[3])\n",
    "        next_state = ((next_state - self.obs_rms.mean.to(self.device)) / torch.sqrt(self.obs_rms.var.to(self.device))).clip(-5, 5)\n",
    "        with torch.no_grad():\n",
    "            target_features = self.target_model(next_state)\n",
    "            features = self.predict_model(next_state)\n",
    "        rewards_int = ((features - target_features)**2).mean(-1)\n",
    "        return rewards_int.reshape(-1)\n",
    "\n",
    "    def learn(self):\n",
    "        # get all data\n",
    "        states, actions, next_states, rewards, dones, old_logits, old_values, old_values_int = self.memory.get_data()\n",
    "\n",
    "        # predict next_value and next_value_int for calculate advantage and td target\n",
    "        targets = []\n",
    "        targets_int = []\n",
    "        with torch.no_grad():\n",
    "            _, next_value, next_value_int = self.model(torch.tensor(np.array(next_states[-1]), device = self.device))\n",
    "        target = next_value\n",
    "        target_int = next_value_int\n",
    "        advantage = 0\n",
    "        advantage_int = 0\n",
    "\n",
    "        # calculate intrinsic rewards, after get intrinsic rewards for all next_state, we need normalize intrinsic rewards. The best way to normalize intrinsic rewards is (min, max) normalizazion (I find this at https://opendilab.github.io/DI-engine/12_policies/rnd.html).\n",
    "        rewards_int = []\n",
    "        with torch.no_grad():\n",
    "            for next_state in np.transpose(np.array(next_states), axes = [1, 0, 2, 3, 4]):\n",
    "                reward_int = self.compure_reward_int(next_state)\n",
    "                reward_int = reward_int.cpu().numpy().tolist()\n",
    "                rewards_int.append(reward_int)\n",
    "        rewards_int = np.transpose(np.array(rewards_int), axes = [1, 0])\n",
    "        rewards_int = (rewards_int - rewards_int.min()) / (rewards_int.max() - rewards_int.min() + 1e-11)\n",
    "        rewards_int = rewards_int.astype(np.float32)\n",
    "\n",
    "        # calculate advantage and td target. We need calculate for both reward and intrinsic rewards.\n",
    "        for state, next_state, reward, reward_int, done, value, value_int in zip(states[::-1], next_states[::-1], rewards[::-1], rewards_int[::-1], dones[::-1], old_values[::-1], old_values_int[::-1]):\n",
    "            done = torch.tensor(done, device = self.device, dtype = torch.float).reshape(-1, 1)\n",
    "            reward = torch.tensor(reward, device = self.device).reshape(-1, 1)\n",
    "            reward_int = torch.tensor(reward_int, device = self.device).reshape(-1, 1)\n",
    "\n",
    "            target = next_value * self.gamma * (1-done) + reward\n",
    "            advantage = target + self.gamma * advantage * (1-done) * self.gae_lambda\n",
    "            targets.append(advantage)\n",
    "            advantage = advantage - value.detach()\n",
    "            next_value = value.detach()\n",
    "\n",
    "            target_int = next_value_int * self.gamma_int * (1-done) + reward_int\n",
    "            advantage_int = target_int + self.gamma_int * advantage_int * (1-done) * self.gae_lambda\n",
    "            targets_int.append(advantage_int)\n",
    "            advantage_int = advantage_int - value_int.detach()\n",
    "            next_value_int = value_int.detach()\n",
    "\n",
    "        # convert all data to tensor\n",
    "        targets = targets[::-1]\n",
    "        targets_int = targets_int[::-1]\n",
    "\n",
    "        action_index = torch.flatten(torch.tensor(actions, device = self.device, dtype = torch.int64))\n",
    "        states = torch.tensor(np.array(states), device = self.device)\n",
    "        states = states.reshape((-1,  states.shape[2], states.shape[3], states.shape[4]))\n",
    "\n",
    "        old_values = torch.cat(old_values, 0)\n",
    "        old_values_int = torch.cat(old_values_int, 0)\n",
    "\n",
    "        targets = torch.cat(targets, 0).view(-1, 1)\n",
    "        targets_int = torch.cat(targets_int, 0).view(-1, 1)\n",
    "\n",
    "        old_logits = torch.cat(old_logits, 0)\n",
    "        old_probs = torch.softmax(old_logits, -1)\n",
    "        index = torch.arange(0, len(old_probs), device = self.device)\n",
    "        old_log_probs = (old_probs[index, action_index] + 1e-9).log()\n",
    "        advantages = (targets - old_values).reshape(-1)\n",
    "        advantages_int = (targets_int - old_values_int).reshape(-1)\n",
    "\n",
    "        early_stopping = False\n",
    "\n",
    "        # update observation running mean std\n",
    "        next_states = torch.tensor(np.array(next_states), device = self.device)\n",
    "        next_states = next_states.reshape(-1, next_states.shape[2], next_states.shape[3], next_states.shape[4])\n",
    "        self.obs_rms.update(next_states.cpu()[:, 3, :, :].reshape(-1, 1, next_states.shape[2], next_states.shape[3]))\n",
    "\n",
    "        #train num_epoch time\n",
    "        for epoch in range(self.num_epoch):\n",
    "            #shuffle data for each epoch\n",
    "            shuffle_ids = torch.randperm(len(targets), dtype = torch.int64)\n",
    "            for i in range(len(old_values)//self.batch_size):\n",
    "                #train with batch_size data\n",
    "                self.optimizer.zero_grad()\n",
    "                self.feature_optimizer.zero_grad()\n",
    "                start_id = i * self.batch_size\n",
    "                end_id = min(len(shuffle_ids), (i+1) * self.batch_size)\n",
    "                batch_ids = shuffle_ids[start_id:end_id]\n",
    "\n",
    "                #predict logits and values from model\n",
    "                logits, value, value_int = self.model(states[batch_ids])\n",
    "\n",
    "                #calculate entropy and value loss (using mse or huber based on config)\n",
    "                probs =  torch.softmax(logits, -1)\n",
    "                entropy = (- (probs * (probs + 1e-9).log()).sum(-1)).mean()\n",
    "                if self.V_loss_type == 'huber':\n",
    "                    loss_V = F.smooth_l1_loss(value, targets[batch_ids])\n",
    "                    loss_V_int = F.smooth_l1_loss(value_int, targets_int[batch_ids])\n",
    "                else:\n",
    "                    loss_V = F.mse_loss(value, targets[batch_ids])\n",
    "                    loss_V_int = F.mse_loss(value_int, targets_int[batch_ids])\n",
    "                index = torch.arange(0, len(probs), device = self.device)\n",
    "                batch_action_index = action_index[batch_ids]\n",
    "\n",
    "                log_probs = (probs[index, batch_action_index] + 1e-9).log()\n",
    "\n",
    "                #approx_kl_div copy from https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/ppo/ppo.html#PPO\n",
    "                #if approx_kl_div larger than 1.5 * target_kl (if target_kl in config is not None), stop training because policy change so much\n",
    "                with torch.no_grad():\n",
    "                    log_ratio = log_probs - old_log_probs[batch_ids]\n",
    "                    approx_kl_div = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    early_stopping = True\n",
    "\n",
    "                #calculate policy loss\n",
    "                ratio = torch.exp(log_probs - old_log_probs[batch_ids])\n",
    "\n",
    "                batch_advantages = self.ext_adv_coef * advantages[batch_ids].detach() + self.int_adv_coef * advantages_int[batch_ids].detach()\n",
    "                if self.is_normalize_advantage:\n",
    "                    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-9)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages\n",
    "                loss_P = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # calculate random distillation network loss\n",
    "                batch_next_states = next_states[batch_ids]\n",
    "                batch_next_states = batch_next_states[:, 3, :, :].reshape(-1, 1, batch_next_states.shape[2], batch_next_states.shape[3])\n",
    "                batch_next_states = ((batch_next_states - self.obs_rms.mean.to(self.device)) / torch.sqrt(self.obs_rms.var.to(self.device))).clip(-5, 5)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    target_features = self.target_model(batch_next_states)\n",
    "                features = self.predict_model(batch_next_states)\n",
    "\n",
    "                update_proportion = 0.25\n",
    "                mask = torch.rand(len(features)).to(self.device)\n",
    "                mask = (mask < update_proportion).type(torch.FloatTensor).to(self.device)\n",
    "                loss_RND = ((features - target_features)**2).mean(-1)\n",
    "                loss_RND = (loss_RND * mask).sum() / torch.max(mask.sum(), torch.Tensor([1]).to(self.device))\n",
    "\n",
    "                # update model\n",
    "                loss = loss_V * self.V_coef + loss_V_int * self.V_coef + loss_P - entropy * self.entropy_coef\n",
    "\n",
    "                self.update_loss_statis(loss_P.item(), loss_V.item(), loss_V_int.item(), entropy.item(), loss.item(), approx_kl_div.item())\n",
    "\n",
    "                if early_stopping == False:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    loss_RND.backward()\n",
    "                    self.feature_optimizer.step()\n",
    "                else:\n",
    "                    break\n",
    "            if early_stopping:\n",
    "                break\n",
    "\n",
    "    def train(self):\n",
    "        episode_reward = [0] * self.num_envs\n",
    "        episode_step = [0] * self.num_envs\n",
    "        max_episode_reward = 0\n",
    "        max_episode_step = 0\n",
    "        episode_time = [datetime.now() for _ in range(self.num_envs)]\n",
    "        total_time = datetime.now()\n",
    "\n",
    "        last_episode_rewards = []\n",
    "\n",
    "        #reset envs\n",
    "        states = self.envs.reset()\n",
    "\n",
    "        while True:\n",
    "            # finish training if agent reach total_step or total_episode base on what type of total_step_or_episode is step or episode\n",
    "            self.current_step += 1\n",
    "\n",
    "            if self.total_step_or_episode == 'step':\n",
    "                if self.current_step >= self.total_step:\n",
    "                    break\n",
    "            else:\n",
    "                if self.current_episode >= self.total_episode:\n",
    "                    break\n",
    "\n",
    "            actions, logit, value, value_int = self.select_action(states)\n",
    "\n",
    "            next_states, rewards, dones, truncs, infos = self.envs.step(actions)\n",
    "\n",
    "            # save to memory\n",
    "            self.memory.save(states, actions, rewards, next_states, dones, logit, value, value_int)\n",
    "\n",
    "            episode_reward = [x + reward for x, reward in zip(episode_reward, rewards)]\n",
    "            episode_step = [x+1 for x in episode_step]\n",
    "\n",
    "            # logging after each step, if 1 episode is ending, I will log this to logging.txt\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    self.current_episode += 1\n",
    "                    max_episode_reward = max(max_episode_reward, episode_reward[i])\n",
    "                    max_episode_step = max(max_episode_step, episode_step[i])\n",
    "                    last_episode_rewards.append(episode_reward[i])\n",
    "                    f_out = open(f\"logging.txt\", \"a\")\n",
    "                    f_out.write(f'episode: {self.current_episode} agent: {i} rewards: {episode_reward[i]:.4f} steps: {episode_step[i]} complete: {infos[i][\"flag_get\"]==True} mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean():.4f} max_rewards: {max_episode_reward:.4f} max_steps: {max_episode_step} current_step: {self.current_step} loss_p: {(self.P_loss.sum()/self.len_loss):.4f} loss_v: {(self.V_loss.sum()/self.len_loss):.4f} loss_v_int: {(self.V_int_loss.sum()/self.len_loss):.4f} loss_e: {(self.E_loss.sum()/self.len_loss):.4f} loss: {(self.total_loss.sum()/self.len_loss):.4f} approx_kl_div: {(self.approx_kl_divs.sum()/self.len_loss):.4f} episode_time: {datetime.now() - episode_time[i]} total_time: {datetime.now() - total_time}\\n')\n",
    "                    f_out.close()\n",
    "                    episode_reward[i] = 0\n",
    "                    episode_step[i] = 0\n",
    "                    episode_time[i] = datetime.now()\n",
    "\n",
    "            # training agent every learn_step\n",
    "            if self.current_step % self.learn_step == 0:\n",
    "                self.learn()\n",
    "                self.memory.reset()\n",
    "\n",
    "            # eval agent every save_figure_step\n",
    "            if self.current_step % self.save_figure_step == 0:\n",
    "                self.save_figure(is_training=True)\n",
    "                if self.is_completed:\n",
    "                    return\n",
    "\n",
    "            if self.current_step % self.save_model_step == 0:\n",
    "                self.save_model()\n",
    "\n",
    "            states = list(next_states)\n",
    "\n",
    "        f_out = open(f\"logging.txt\", \"a\")\n",
    "        f_out.write(f' mean_rewards: {np.array(last_episode_rewards[-min(len(last_episode_rewards), 100):]).mean()} max_rewards: {max_episode_reward} max_steps: {max_episode_step} current_step: {self.current_step} total_time: {datetime.now() - total_time}\\n')\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8b34d",
   "metadata": {},
   "source": [
    "# Create model\n",
    "## Actor Critic Model\n",
    "I use the same model architecture as [PPO](https://github.com/CVHvn/Mario_PPO).\n",
    "Model includes:\n",
    "- 4 convolution layers to encode input image (observation) to feature vector.\n",
    "- 1 hidden linear layer.\n",
    "- two linear layers for policy and value prediction (actor and critic).\n",
    "\n",
    "## RND model\n",
    "Both networks have same architecture:\n",
    "- 4 convolution layers to encode input image (observation) to feature vector.\n",
    "- 1 or 2 hidden linear layers:\n",
    "    - 1 hidden linear layer for target network (random network or non-learning network).\n",
    "    - 2 hidden linear layers for feature network (learning network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb",
   "metadata": {
    "id": "dee1e253-b83d-48d5-aa9c-4463303c3feb"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim[0], 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.linear = nn.Linear(1152, 512)\n",
    "        self.int_critic = nn.Linear(512, 1)\n",
    "        self.ext_critic = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x/255.))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return self.actor_linear(x), self.ext_critic(x), self.int_critic(x)\n",
    "\n",
    "class Feature_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Feature_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.linear1 = nn.Linear(1152, 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class Target_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Target_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.linear = nn.Linear(1152, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390964d",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f00225",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config.state_dim, config.action_dim)\n",
    "target_model = Target_Model(config.state_dim, config.action_dim)\n",
    "predict_model = Feature_Model(config.state_dim, config.action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a241d-419a-4f72-9f2c-7356d7858eae",
   "metadata": {
    "id": "077a241d-419a-4f72-9f2c-7356d7858eae"
   },
   "outputs": [],
   "source": [
    "envs = MultipleEnvironments(config.world, config.stage, config.action_type, config.num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539",
   "metadata": {
    "id": "ae4f5fde-6390-4a81-bb29-223fa70e9539"
   },
   "outputs": [],
   "source": [
    "agent = Agent(envs = envs, world = config.world, stage = config.stage, action_type = config.action_type, num_envs = config.num_envs, \n",
    "              state_dim = config.state_dim, action_dim = config.action_dim, save_dir = config.save_dir,\n",
    "              save_model_step = config.save_model_step, save_figure_step = config.save_figure_step, learn_step = config.learn_step,\n",
    "              total_step_or_episode = config.total_step_or_episode, total_step = config.total_step, total_episode = config.total_episode,\n",
    "              model = model, target_model = target_model, predict_model = predict_model, gamma = config.gamma, gamma_int = config.gamma_int,\n",
    "              learning_rate = config.learning_rate, entropy_coef = config.entropy_coef, V_coef = config.V_coef,\n",
    "              max_grad_norm = config.max_grad_norm, clip_param = config.clip_param, batch_size = config.batch_size,\n",
    "              num_epoch = config.num_epoch, is_normalize_advantage = config.is_normalize_advantage, V_loss_type = config.V_loss_type,\n",
    "              target_kl = config.target_kl, gae_lambda = config.gae_lambda, ext_adv_coef = config.ext_adv_coef,\n",
    "              int_adv_coef = config.int_adv_coef, device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca",
   "metadata": {
    "id": "5f94e0c8-e8c7-4f37-ae4b-ffc9083b6aca"
   },
   "outputs": [],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e1069",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb568ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model(\"best_model.pth\")\n",
    "agent.save_figure()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
